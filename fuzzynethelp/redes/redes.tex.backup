\documentclass[letterpaper,12pt]{book}
\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}
\usepackage{pst-all}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\title{Redes de Cálculo con Palabras}
\author{Oscar G. Duarte V.}
\date{}
\begin{document}
%\maketitle
%\tableofcontents

\chapter{Introducción a las Redes de Cálculo con Palabras}
Dado que los Sistemas de Computación con palabras aceptan números difusos como entradas y salidas válidas, podemos concatenar varios de estos sistemas. Con está idea proponemos construir Redes como la que se muestra en la figura \ref{fig:red_gral}. 

\begin{figure}
	\centering
    \fcolorbox{white}{white}{
	\begin{pspicture}(0,0)(7,4)%\psgrid
	\cnode(3.5,3.5){.4cm}{N1}
	\cnode(2,2){.4cm}{N11}
	\pnode(3.5,2.2){N12} \rput[c](3.5,2){$\cdots$}
	\cnode(5,2){.4cm}{N13}
	\cnode(1,.5){.4cm}{N21}
	\pnode(2,.7){N22} \rput[c](2,.5){$\cdots$}
	\cnode(3,.5){.4cm}{N23}
	\cnode(4,.5){.4cm}{N24}
	\pnode(5,.7){N25} \rput[c](5,.5){$\cdots$}
	\cnode(6,.5){.4cm}{N26}
	\ncline{->}{N11}{N1}
	\ncline{->}{N12}{N1}
	\ncline{->}{N13}{N1}
	\ncline{->}{N21}{N11}
	\ncline{->}{N22}{N11}
	\ncline{->}{N23}{N11}
	\ncline{->}{N24}{N13}
	\ncline{->}{N25}{N13}
	\ncline{->}{N26}{N13}
	\end{pspicture}
    }
	\caption{Redes de Sistemas de Computación con Palabras}
	\label{fig:red_gral}
\end{figure}

Se trata de una red orientada \textit{hacia adelante}, que puede organizarse en capas; la salida de un determinado nodo puede ser la entrada de otro nodo de capas \textit{más adelante} (no necesariamente de la capa siguiente). Cada nodo contiene un sistema de computación con palabras asociado, y las señales que conectan nodos son Números Difusos.

Las reglas de evaluación de la red son las siguientes
\begin{enumerate}
\item Cualquier Nodo de la red puede definirse como una entrada o una salida del sistema. 
\item Si un Nodo se define como \textit{Salida}, ésta será cualquiera de las salidas válidas de los Sistemas de Computación con palabras.
\item Si un Nodo se define como \textit{Entrada}, ésta será cualquiera de las entradas válidas de los Sistemas de Computación con palabras.
\item Si un Nodo no recibe señales de ningún otro nodo, debe definirse como Entrada del sistema.
\item Si un Nodo se define como Entrada del sistema, su valor será la entrada \textit{actual}. En caso contario, su valor será la salida del Sistema de Computación con palabras asociado al nodo
\end{enumerate}

\chapter{Entrenamiento de las Redes}
Un tipo especial de redes podría ser "multicapa hacia adelante", y en cada nodo podría haber una función de razonamiento aproximado que fuese una función logística aplicada sobre la suma ponderada de las entradas. En esas condiciones, la red sería similar a una Red Neuronal típica, operando sobre palabras (realmente sobre números difusos). Parecería sensato proponer un algoritmo de entrenamiento similar al de las redes convencionales: retropropagación del error. Sin embargo, esto no es inmediato, como se muestra a continuación.

\section{Funciones de Error}

Sea $Y_r$ el valor calculado por la red, y $Y_d$ el valor deseado. Sean $[L_{Y_r}(\alpha), R_{Y_r}(\alpha)]$ y $[L_{Y_d}(\alpha),R_{Y_d}(\alpha)]$ sus $\alpha$-cortes. Supóngase además que se tiene una representación discreta de los números difusos, para valores de $\alpha = \alpha_0,\cdots,\alpha_A$. Podemos definir las funciones de error $\epsilon_1$ y $\epsilon_2$ asi
\[
\Delta_L(\alpha_i)=(L_{Y_r}(\alpha_i)-L_{Y_d}(\alpha_i))^2
\]
\[
\Delta_R(\alpha_i)=(R_{Y_r}(\alpha_i)-R_{Y_d}(\alpha_i))^2
\]
\[
\varepsilon(\alpha_i)=\frac{1}{2}(\Delta_L(\alpha_i)+\Delta_R(\alpha_i))
\]
\[
\epsilon_1=\frac{1}{2}\sum_{i=0}^{A}\varepsilon(\alpha_i)
\] 
\[
\epsilon_2=\frac{1}{2}\sum_{i=0}^{A}\alpha_i\varepsilon(\alpha_i)
\] 

Otra alternativa consiste en calcular un valor representativo de los número $Y_r$ y $Y_d$ (ver seccion \ref{sec:valor}) y compararlos para definir el error $\epsilon_3$. Sean $\bar Y_r$ y $\bar Y_d$ esos valores:
\[
\epsilon_3=(\bar Y_r - \bar Y_d)^2
\]

\section{Diferenciabilidad}
Para emplear el algoritmo de retropropagación es necesario calcular el gradiente de la función de error respecto a las variables a ajustar. Supóngase un nodo de la red de en la última capa, cuya función de razonameinto aproximado sea 
\[
y=\phi(\nu)=\frac{1}{1+e^{-a\nu}}
\]
\[
\nu=\sum_{j=1}^{n}{\omega_j x_j}
\]
En donde $y$ es la salida, $x_j$ son las entradas, y $\omega_j$ los pesos a ajustar.

Si empleamos como función de error a $\epsilon_1$, el gradiente descendiente estará formado por los términos $\frac{\partial \epsilon_1}{\partial \omega_j}$ que se calcularían asi:
\[
\frac{\partial \epsilon_1}{\partial \omega_j}=
\sum_{i=0}^{A}\frac{\partial \varepsilon(\alpha_i)}{\partial \omega_j}
\]
\[
\frac{\partial \varepsilon(\alpha_i)}{\partial \omega_j}=
\frac{\partial \varepsilon(\alpha_i)}{\partial L_{Y_r}(\alpha_i)}
\frac{\partial L_{Y_r}(\alpha_i)}{\partial \omega_j} +
\frac{\partial \varepsilon(\alpha_i)}{\partial R_{Y_r}(\alpha_i)}
\frac{\partial R_{Y_r}(\alpha_i)}{\partial \omega_j}
\]
\[
\frac{\partial \varepsilon(\alpha_i)}{\partial L_{Y_r}(\alpha_i)}=(L_{Y_r}(\alpha)-L_{Y_d}(\alpha))
\]
\[
\frac{\partial \varepsilon(\alpha_i)}{\partial R_{Y_r}(\alpha_i)}=(R_{Y_r}(\alpha)-R_{Y_d}(\alpha))
\]
Como $\phi$ es una función monotonamente creciente, entonces
\[
\frac{\partial L_{Y_r}(\alpha_i)}{\partial \omega_j}=
\frac{\partial L_{Y_r}(\alpha_i)}{\partial L_\nu(\alpha_i)}\frac{\partial L_\nu(\alpha_i)}{\partial \omega_j}
\]
\[
\frac{\partial R_{Y_r}(\alpha_i)}{\partial \omega_j}=
\frac{\partial R_{Y_r}(\alpha_i)}{\partial R_\nu(\alpha_i)}\frac{\partial R_\nu(\alpha_i)}{\partial \omega_j}
\]

Pero las derivadas parciales respecto a $\omega_j$ dependen del signo de $\omega_j$:
\[
\frac{\partial L_\nu(\alpha_i)}{\partial \omega_j}=
\left\{
\begin{matrix}
L_{x_j}(\alpha_i) & \text{si } \omega_j > 0 \\
R_{x_j}(\alpha_i) & \text{si } \omega_j < 0 \\
\text{indefinida} & \text{si } \omega = 0
\end{matrix}
\right.
\]

\[
\frac{\partial R_\nu(\alpha_i)}{\partial \omega_j}=
\left\{
\begin{matrix}
R_{x_j}(\alpha_i) & \text{si } \omega_j > 0 \\
L_{x_j}(\alpha_i) & \text{si } \omega_j < 0 \\
\text{indefinida} & \text{si } \omega = 0
\end{matrix}
\right.
\]

Estas discontinuidades plantean una seria dificultad para el método del gradiente descendiente. Este problema no se resuelve al tomar $\epsilon_2$ ni $\epsilon_3$.

Posibles alternativas de solución (no han sido evaluadas):
\begin{enumerate}
\item Asegurar que $\omega_j>0$. En este caso sólo podrian representarse funciones monótonamente crecientes.
\item Asegurar que $\omega_j>0$ y tomar el doble de entradas: por cada $x_j$ crear $-x_j$
\item Emplear alguna heurística para manipular el cambio de signo de $\omega_j$
\item Emplear Algoritmos Genéticos para encontrar los $\omega_j$
\end{enumerate}

\chapter{Valor Representativo de un Número Difuso. Ordenamiento}
\label{sec:valor}

Una de las estrategias de ordenamiento de números difusos consiste en caracterizar cada número difuso por un \textit{valor representativo} o índice, y emplear el ordenamiento natural de los reales. 

De la múltiples propuestas que existen para calcular ese valor representativo, presentamos aqui una ligera variación sobre la propuesta de Delgado y Vila.

Sea $N$ un número difuso, y $N_\alpha=[L_\alpha, R_\alpha]$ un $\alpha-$corte. Definimos $v_N(\alpha,\beta)$ el valor representativo de $N_\alpha$  como
\begin{equation}
	v_N(\alpha,\beta)=(1-\beta)L_\alpha + \beta R_\alpha\qquad 0\leq\beta\leq 1
	\label{equ:v_alpha}
\end{equation}

Claramente se ve que $v_N(\alpha,\beta) \in N_\alpha$; además, si variamos $\beta$ desde $0$ hasta $1$ entonces $v_N(\alpha,\beta)$ ``viaja'' desde $L_\alpha$ hasta $R_\alpha$. Denominamos a $\beta$ el \textit{optimismo} de la representación crisp que hacemos del intervalo.

Sea $g(\alpha):[0,1]\to [0,1]$ una función creciente tal que $g(0)=0$ y $g(1)=1$. Definimos $V_N(\beta)$, el valor representativo de $N$ como el promedio ponderado por $g(\alpha)$ de los valores representativos de todos los $\alpha-$cortes:
\begin{equation}
	V_N(\beta)=\frac{\int_0^1 g(\alpha)v_N(\alpha,\beta)d\alpha}
	{\int_0^1 g(\alpha)d\alpha}
	\label{equ:v_N}
\end{equation}

\section{Primer Caso}
Tomemos $g(\alpha)=\alpha^r$, $r>0$ y N el número trapezoidal $T(a,b,c,d)$
tenemos entonces:
\[
L_\alpha=a+\alpha(b-a) \quad
R_\alpha=c+\alpha(d-c) 
\]

\[
v_N(\alpha,\beta)=(1-\beta)[a+\alpha(b-a)] + \beta[c+\alpha(d-c)]
\]

Dado que $V_N(0,\beta)=(1-\beta)a+\beta c$ y que $V_N(1,\beta)=(1-\beta)b+\beta d$ entonces tenemos

\[
v_N(\alpha,\beta)=v_N(0,\beta)+\alpha[v_N(1,\beta)-v_N(0,\beta)]
\]

\begin{equation}
	v_N(\alpha,\beta)=(1-\alpha)v_N(0,\beta)+\alpha v_N(1,\beta)
	\label{equ:v_alpha_eje_1}
\end{equation}

Es decir, en un número trapezoidal el valor representativo de cada $\alpha-$corte  puede calcularse como la interpolación lineal entre los valores representativos de los $\alpha-$cortes correspondientes a $\alpha=0$ y  $\alpha=1$.

Ahora tenemos:
\[
V_N(\beta)=\frac{\int_0^1 \alpha^rv_N(\alpha,\beta)d\alpha}
	{\int_0^1 \alpha^r d\alpha}
\]
\[
V_N(\beta)=(r+1)\int_0^1 \alpha^rv_N(\alpha,\beta)d\alpha
\]
\[
V_N(\beta)=v_N(0,\beta)+
\frac{r+1}{r+2}\left[v_N(1,\beta)-v_N(0,\beta)\right]
\]

\begin{equation}
	V_N(\beta)=v_N\left(\frac{r+1}{r+2},\beta\right)
	\label{equ:v_N_eje_1}
\end{equation}

Es decir, en este caso el valor representativo del número difuso es el valor representativo del $\alpha-$corte correspondiente a $\hat\alpha=\frac{r+1}{r+2}$. Nótese que $1/2\leq\hat\alpha\leq 1$

\textbf{Ejemplo:} Dados dos número trapezoidales $N_1=T(2,3,3,4)$ y $N_2=T(1,3,3,5)$, ¿cuál de los dos números es mayor?. Seleccionemos $r=0$...

La respuesta depende del optimismo $\beta$. Por ejemplo, si $\beta=0$ tenemos $V_{N_1}=2.5$ y $V_{N_2}=2.0$, por lo tanto $N_2<N_1$. Sin embargo, si $\beta=1$ tenemos $V_{N_1}=3.5$ y $V_{N_2}=4.0$ y por lo tanto $N_1<N_2$. Si tomamos $\beta=0.5$ tenemos $V_{N_1}=V_{N_2}=3.0$ y diriamos que ninguno de los números es mayor que el otro.

\section{Segundo Caso}
Tomemos $g(\alpha)=\alpha^r$, $r>0$ y N un número difuso cuya función de pertenencia está formada por trazos rectos entre $\alpha_i$ y $\alpha_i+1$ con $i=0,1,\cdots A-1$. En ese caso tendremos
\[
\int_0^1 \alpha^rv_N(\alpha,\beta)d\alpha=
\sum_{i=0}^{A-1}\int_{\alpha_i}^{\alpha_{i+1}} \alpha^rv_N(\alpha,\beta)d\alpha
\]
Para el cálculo de cada integral seguimos un procedimiento semejante al del caso 1, tomando $a,b,c,d$ tales que
\[
L_{\alpha_i}=a \quad R_{\alpha_i}=d \quad
L_{\alpha_{i+1}}=b \quad R_{\alpha_{i+1}}=c
\]
Por lo tanto, para $\alpha_i \leq \alpha \leq \alpha_{i+1}$
\[
L_\alpha=a+\frac{(\alpha-\alpha_i)}{(\alpha_{i+1}-\alpha_i)}(b-a) \quad
R_\alpha=c+\frac{(\alpha-\alpha_i)}{(\alpha_{i+1}-\alpha_i)}(d-c) 
\]

\end{document}
